import re
import unicodedata
from collections import Counter
from copy import deepcopy
from typing import List

import numpy as np

from lm_eval.api.instance import Instance
from lm_eval.api.task import ConfigurableTask


class IndicQA_Gen(ConfigurableTask):
    """
    IndicQA generation task for multilingual Indian languages.
    Similar to SQuAD completion but uses the ai4bharat/IndicQA dataset.

    """
    VERSION = 1
    DATASET_PATH = "hf://datasets/ai4bharat/IndicQA"
    DATASET_NAME = None  # Will be set by language-specific subclasses

    COMMON_CONFIG = {
        "metadata": {"version": VERSION},
        "task": "indic_qa_gen",
        "tag": "indic_qa_gen",
        "dataset_path": DATASET_PATH,
        "output_type": "generate_until",
    }


    def __init__(self, config=None):
        super().__init__(config=config)

    def has_training_docs(self):
        return False  # IndicQA only has test split

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return []

    def validation_docs(self):
        return []

    def test_docs(self):
        """Return test documents from the dataset"""
        return self.dataset["test"]

    def doc_to_text(self, doc):
        """
        Format: Context: [passage] \n Question: [question] \n Answer:
        """
        context = doc["context"]
        question = doc["question"]
        return f"Context: {context}\nQuestion: {question}\nAnswer:"

    def doc_to_target(self, doc):
        """Extract the answer text from the answers structure"""
        answers = doc["answers"]
        
        # IndicQA format: answers is a list of dicts with 'text' and 'answer_start'
        if isinstance(answers, list) and len(answers) > 0:
            if isinstance(answers[0], dict) and "text" in answers[0]:
                answer_text = answers[0]["text"]
                # Handle empty answers (unanswerable questions)
                if answer_text == "" or answer_text is None:
                    return ""
                return answer_text
            elif isinstance(answers[0], str):
                return answers[0]
        
        # Fallback - return empty string if structure is unexpected
        return ""

    def construct_requests(
        self, doc, ctx, chat_template=None, apply_chat_template=False, **kwargs
    ):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        arguments = deepcopy(self.config.generation_kwargs) if hasattr(self.config, 'generation_kwargs') and self.config.generation_kwargs else {}
        arguments["until"] = arguments.get("until", ["\n", "Context:", "Question:"])
        arguments["max_gen_toks"] = arguments.get("max_gen_toks", 64)
        return [
            Instance(
                request_type="generate_until",
                doc=doc,
                arguments=(ctx, arguments),
                idx=0,
                **kwargs,
            )
        ]

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        continuation = results
        target = self.doc_to_target(doc)
        
        # Get all possible answer texts for comparison
        answers = doc["answers"]
        answer_list = []
        
        if isinstance(answers, list):
            for ans in answers:
                if isinstance(ans, dict) and "text" in ans:
                    answer_text = ans["text"]
                    if answer_text and answer_text != "":
                        answer_list.append(answer_text)
                elif isinstance(ans, str) and ans != "":
                    answer_list.append(ans)
        
        if not answer_list:
            answer_list = [target] if target else [""]
        
        # Calculate metrics
        prediction_text = continuation[0]

        # Calculate F1 score (max over all possible answers)
        f1_scores = [qa_f1_score(prediction_text, answer) for answer in answer_list if answer]
        max_f1 = max(f1_scores) if f1_scores else 0.0

        # Calculate Exact Match (1 if any answer matches exactly after normalization)
        em_scores = [exact_match_score(prediction_text, answer) for answer in answer_list if answer]
        max_em = max(em_scores) if em_scores else 0.0

        # Calculate Contains (1 if answer appears in prediction)
        contains = contains_score(prediction_text, answer_list)

        return {
            "f1": max_f1,
            "exact_match": max_em,
            "contains": contains
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "f1": np.mean,  # Average F1 score across examples
            "exact_match": np.mean,  # Exact match accuracy
            "contains": np.mean,  # Contains match (answer appears in generated text)
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "f1": True,  # Higher F1 is better
            "exact_match": True,  # Higher EM is better
            "contains": True,  # Contains match (answer appears in generated text)
        }


class IndicQA_Gen_Lang(IndicQA_Gen):
    """Base class for language-specific IndicQA generation tasks"""
    
    LANG = None  # To be overridden by subclasses
    
    def __init__(self, config=None):
        import copy

        lang_config = copy.deepcopy(self.COMMON_CONFIG)
        lang_config["task"] = f"indic_qa_gen_{self.LANG}"
        lang_config["dataset_name"] = self.LANG

        super().__init__(config=lang_config)

    def task_lang(self):
        return self.LANG


# Language-specific classes (IndicQA supports 11 languages, no English)
class IndicQA_Gen_Hi(IndicQA_Gen_Lang):
    LANG = "hi"

class IndicQA_Gen_As(IndicQA_Gen_Lang):
    LANG = "as"

class IndicQA_Gen_Bn(IndicQA_Gen_Lang):
    LANG = "bn"

class IndicQA_Gen_Gu(IndicQA_Gen_Lang):
    LANG = "gu"

class IndicQA_Gen_Kn(IndicQA_Gen_Lang):
    LANG = "kn"

class IndicQA_Gen_Ml(IndicQA_Gen_Lang):
    LANG = "ml"

class IndicQA_Gen_Mr(IndicQA_Gen_Lang):
    LANG = "mr"

class IndicQA_Gen_Or(IndicQA_Gen_Lang):
    LANG = "or"

class IndicQA_Gen_Pa(IndicQA_Gen_Lang):
    LANG = "pa"

class IndicQA_Gen_Ta(IndicQA_Gen_Lang):
    LANG = "ta"

class IndicQA_Gen_Te(IndicQA_Gen_Lang):
    LANG = "te"


def normalize_answer(s: str) -> str:
    """Normalize answer string for comparison.

    This normalization is standard for SQuAD-style QA tasks:
    - Lowercase
    - Remove punctuation
    - Remove extra whitespace
    - Unicode normalization
    """

    if not s:
        return ""

    # Unicode normalization
    def unicode_normalize(text):
        return unicodedata.normalize("NFC", text)

    # Normalize whitespace
    def normalize_whitespace(text):
        return re.sub(r"\s+", " ", text).strip()

    # Remove punctuation
    def remove_punc(text):
        return re.sub(r"[।,!?\"“”‘’():;]", " ", text)

    # Lowercase
    def lower(text):
        return text.lower()

    return normalize_whitespace(remove_punc(lower(unicode_normalize(s))))


def exact_match_score(prediction: str, ground_truth: str) -> float:
    """Calculate exact match score after normalization.

    Returns 1.0 if normalized strings match exactly, 0.0 otherwise.
    """
    return float(normalize_answer(prediction) == normalize_answer(ground_truth))


def qa_f1_score(prediction: str, ground_truth: str) -> float:
    """Calculate token-level F1 score between prediction and ground truth.

    This is the standard F1 metric used in SQuAD and similar QA tasks.
    """
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()

    # Handle empty cases
    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:
        return float(prediction_tokens == ground_truth_tokens)

    # Calculate token overlap
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())

    if num_same == 0:
        return 0.0

    precision = num_same / len(prediction_tokens)
    recall = num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)

    return f1


def contains_score(prediction: str, labels: List[str]) -> float:
    """Check if any of the labels appears in the prediction (case-insensitive)"""
    if not labels:
        return 0.0
    
    return max(
        int(bool(re.search(re.compile(re.escape(label), re.IGNORECASE), prediction)))
        for label in labels
        if label  # Skip empty labels
    )