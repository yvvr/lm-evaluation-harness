import re
from copy import deepcopy
from typing import List

import numpy as np

from lm_eval.api.instance import Instance
from lm_eval.api.task import ConfigurableTask


class IGB_XQuad_IN_Gen(ConfigurableTask):
    """
    IndicGenBench XQuAD-IN generation task.
    Similar to SQuAD completion but for multilingual Indian languages XQuAD dataset.
    """
    VERSION = 1
    DATASET_PATH = "google/IndicGenBench_xquad_in"
    DATASET_NAME = "default"

    COMMON_CONFIG = {
        "metadata": {"version": VERSION},
        "task": "igb_xquad_in_gen",
        "tag": "igb_xquad_in_gen",
        "dataset_path": DATASET_PATH,
        "dataset_kwargs": {"field": "examples"},
        "output_type": "generate_until",
    }

    def __init__(self, config=None):
        super().__init__(config=config)

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        """
        Format: Context: [passage] \n Question: [question] \n Answer:
        """
        context = doc["context"]
        question = doc["question"]
        return f"Context: {context}\nQuestion: {question}\nAnswer:"

    def doc_to_target(self, doc):
        """Extract the answer text from the answers structure"""
        answers = doc["answers"]
        if isinstance(answers, dict) and "text" in answers:
            # Handle case where answers is a dict with 'text' list
            if isinstance(answers["text"], list) and len(answers["text"]) > 0:
                return answers["text"][0]
            elif isinstance(answers["text"], str):
                return answers["text"]
        elif isinstance(answers, list) and len(answers) > 0:
            # Handle case where answers is a list of dicts
            if isinstance(answers[0], dict) and "text" in answers[0]:
                return answers[0]["text"]
            elif isinstance(answers[0], str):
                return answers[0]
        
        # Fallback - return empty string if structure is unexpected
        return ""

    def construct_requests(
        self, doc, ctx, chat_template=None, apply_chat_template=False, **kwargs
    ):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        arguments = deepcopy(self.config.generation_kwargs) if hasattr(self.config, 'generation_kwargs') and self.config.generation_kwargs else {}
        arguments["until"] = arguments.get("until", ["\n", "Context:", "Question:"])
        arguments["max_gen_toks"] = arguments.get("max_gen_toks", 64)
        return [
            Instance(
                request_type="generate_until",
                doc=doc,
                arguments=(ctx, arguments),
                idx=0,
                **kwargs,
            )
        ]

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        continuation = results
        target = self.doc_to_target(doc)
        
        # Get all possible answer texts for comparison
        answers = doc["answers"]
        answer_list = []
        
        if isinstance(answers, dict) and "text" in answers:
            if isinstance(answers["text"], list):
                answer_list = answers["text"]
            else:
                answer_list = [answers["text"]]
        elif isinstance(answers, list):
            for ans in answers:
                if isinstance(ans, dict) and "text" in ans:
                    answer_list.append(ans["text"])
                elif isinstance(ans, str):
                    answer_list.append(ans)
        
        if not answer_list:
            answer_list = [target] if target else [""]
        
        return {
            "contains": contains_score(continuation[0], answer_list)
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "contains": np.mean,  # Contains match (answer appears in generated text)
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "contains": True,  # Contains match (answer appears in generated text)
        }


class IGB_XQuad_IN_Gen_Lang(IGB_XQuad_IN_Gen):
    """Base class for language-specific IGB XQuAD generation tasks"""
    
    LANG = None  # To be overridden by subclasses
    
    def __init__(self, config=None):
        import copy

        lang_config = copy.deepcopy(self.COMMON_CONFIG)
        lang_config["task"] = f"igb_xquad_in_gen_{self.LANG}"

        super().__init__(config=lang_config)

    def task_lang(self):
        return self.LANG
    
    def create_docs(self, split):
        """Filter documents by language"""
        if self.LANG is None:
            return self.dataset[split]
        
        return self.dataset[split].filter(
            lambda example: example["lang"] == self.task_lang(),
            num_proc=8,
            desc=f"Dropping {split} instances whose language is not {self.LANG}",
        )

    def training_docs(self):
        return self.create_docs("train")

    def validation_docs(self):
        return self.create_docs("validation")

    def test_docs(self):
        return self.create_docs("test")


# Language-specific classes
class IGB_XQuad_IN_Gen_Hi(IGB_XQuad_IN_Gen_Lang):
    LANG = "hi"

class IGB_XQuad_IN_Gen_En(IGB_XQuad_IN_Gen_Lang):
    LANG = "en"

class IGB_XQuad_IN_Gen_As(IGB_XQuad_IN_Gen_Lang):
    LANG = "as"

class IGB_XQuad_IN_Gen_Bn(IGB_XQuad_IN_Gen_Lang):
    LANG = "bn"

class IGB_XQuad_IN_Gen_Gu(IGB_XQuad_IN_Gen_Lang):
    LANG = "gu"

class IGB_XQuad_IN_Gen_Kn(IGB_XQuad_IN_Gen_Lang):
    LANG = "kn"

class IGB_XQuad_IN_Gen_Ml(IGB_XQuad_IN_Gen_Lang):
    LANG = "ml"

class IGB_XQuad_IN_Gen_Mr(IGB_XQuad_IN_Gen_Lang):
    LANG = "mr"

class IGB_XQuad_IN_Gen_Or(IGB_XQuad_IN_Gen_Lang):
    LANG = "or"

class IGB_XQuad_IN_Gen_Pa(IGB_XQuad_IN_Gen_Lang):
    LANG = "pa"

class IGB_XQuad_IN_Gen_Ta(IGB_XQuad_IN_Gen_Lang):
    LANG = "ta"

class IGB_XQuad_IN_Gen_Te(IGB_XQuad_IN_Gen_Lang):
    LANG = "te"


def contains_score(prediction: str, labels: List[str]) -> float:
    """Check if any of the labels appears in the prediction (case-insensitive)"""
    if not labels:
        return 0.0
    
    return max(
        int(bool(re.search(re.compile(re.escape(label), re.IGNORECASE), prediction)))
        for label in labels
        if label  # Skip empty labels
    )